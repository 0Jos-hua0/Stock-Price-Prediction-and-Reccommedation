{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Prediction Model Training\n",
    "\n",
    "This notebook contains the complete pipeline for training our LSTM (Long Short-Term Memory) models. For each stock ticker, we will perform the following steps:\n",
    "\n",
    "1.  **Fetch Historical Data**: Download the latest stock data using the `yfinance` library.\n",
    "2.  **Preprocess Data**: Scale the data and create sequences suitable for a time-series model.\n",
    "3.  **Build the LSTM Model**: Define the architecture of our neural network using TensorFlow/Keras.\n",
    "4.  **Train the Model**: Train a unique model on the historical data of each stock.\n",
    "5.  **Save the Model**: Save the trained model to a file, ready to be used by our Flask backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance tensorflow scikit-learn -q\n",
    "\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define Tickers and Fetch Data\n",
    "\n",
    "We'll use the same list of tickers from our `stock_data.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'TSLA', 'JPM', 'JNJ', 'V', 'PG',\n",
    "    'XOM', 'UNH', 'HD', 'MA', 'PFE', 'KO', 'BAC', 'MCD', 'COST', 'WMT',\n",
    "    'CVX', 'LLY', 'PEP', 'DIS'\n",
    "]\n",
    "\n",
    "# Fetch data for the last 5 years\n",
    "end_date = pd.Timestamp.now()\n",
    "start_date = end_date - pd.DateOffset(years=5)\n",
    "\n",
    "print(f\"Fetching data from {start_date.date()} to {end_date.date()}...\")\n",
    "all_data = yf.download(tickers, start=start_date, end=end_date)\n",
    "print(\"Data fetching complete.\")\n",
    "\n",
    "# We'll focus on the 'Close' price for our predictions\n",
    "close_prices = all_data['Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Data Preprocessing\n",
    "\n",
    "We'll create a function to prepare the data for our LSTM. This involves:\n",
    "1.  Scaling the prices between 0 and 1 to help the model train better.\n",
    "2.  Creating sequences of data (e.g., use the last 60 days to predict the 61st day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, time_step=60):\n",
    "    \"\"\"Creates sequences of data for LSTM training.\"\"\"\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        a = data[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the LSTM Model Architecture\n",
    "\n",
    "We'll create a function that defines our LSTM model structure. This makes it easy to create a fresh, untraiend model for each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"Builds and compiles a Keras LSTM model.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(units=50, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=50, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(units=50),\n",
    "        Dropout(0.2),\n",
    "        Dense(units=1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Train and Save a Model for Each Stock\n",
    "\n",
    "This is the main loop. We will iterate through each ticker, prepare its specific data, train a model, and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 60\n",
    "MODELS_DIR = 'trained_models'\n",
    "\n",
    "if not os.path.exists(MODELS_DIR):\n",
    "    os.makedirs(MODELS_DIR)\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f'\\n--- Processing and Training for {ticker} ---')\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    ticker_data = close_prices[ticker].dropna().values.reshape(-1, 1)\n",
    "    if len(ticker_data) < TIME_STEP + 100: # Ensure we have enough data\n",
    "        print(f\"Skipping {ticker} due to insufficient data.\")\n",
    "        continue\n",
    "        \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = scaler.fit_transform(ticker_data)\n",
    "    \n",
    "    X, y = create_dataset(scaled_data, TIME_STEP)\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features]\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    \n",
    "    # 2. Build Model\n",
    "    model = build_model(input_shape=(TIME_STEP, 1))\n",
    "    \n",
    "    # 3. Train Model\n",
    "    print(f'Starting training for {ticker}...')\n",
    "    # Using fewer epochs for a quick demonstration. Increase epochs for better accuracy.\n",
    "    model.fit(X, y, epochs=10, batch_size=32, verbose=0)\n",
    "    print(f'Training finished for {ticker}.')\n",
    "    \n",
    "    # 4. Save Model and Scaler\n",
    "    model_path = os.path.join(MODELS_DIR, f'{ticker}_model.h5')\n",
    "    model.save(model_path)\n",
    "    \n",
    "    # We MUST save the scaler for each stock to correctly inverse the prediction later\n",
    "    import pickle\n",
    "    scaler_path = os.path.join(MODELS_DIR, f'{ticker}_scaler.pkl')\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "        \n",
    "    print(f'Successfully saved model to {model_path} and scaler to {scaler_path}')\n",
    "\n",
    "print('\\n--- All models have been trained and saved! ---')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Next Steps\n",
    "\n",
    "After running this notebook, you will have a folder named `trained_models` containing:\n",
    "1.  An `.h5` file for each stock (the trained neural network).\n",
    "2.  A `.pkl` file for each stock (the scaler used for its data).\n",
    "\n",
    "**To use these in your project:**\n",
    "\n",
    "1.  Click the 'Files' icon on the left sidebar in Colab.\n",
    "2.  You will see the `trained_models` directory. Right-click it and select 'Download'.\n",
    "3.  This will download a `.zip` file. Unzip it.\n",
    "4.  Place the entire `trained_models` folder into your Flask project directory, alongside `app.py`.\n",
    "\n",
    "Our next step will be to update the `get_price_prediction` function in `app.py` to load and use these real models instead of the random placeholder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
